{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train a Model from Scratch and Deploy to Device\r\n",
        "\r\n",
        "This notebook will walk you through all the steps required to train up a brand new model and deploy it to your device.\r\n",
        "As an amusing motivating example, we will do the following:\r\n",
        "\r\n",
        "1. Download COCO dataset.\r\n",
        "1. Use Deep Dream to convert it into a funny-looking dataset.\r\n",
        "1. Train a Pix2Pix GAN generator to convert COCO images into dreamified images.\r\n",
        "1. Convert the generator model into OpenVINO.\r\n",
        "1. Modify the device's azureeyemodule to accept this model.\r\n",
        "1. Download this model to the Percept DK."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the GitHub repo"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topdir = !pwd\r\n",
        "topdir = topdir[0]\r\n",
        "topdir"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616104606543
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/microsoft/azure-percept-advanced-development.git\r\n",
        "%cd azure-percept-advanced-development\r\n",
        "!git checkout strangem-tutorial\r\n",
        "!git pull\r\n",
        "%cd tutorials/pytorch-from-scratch-tutorial"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make the Dataset\r\n",
        "\r\n",
        "To make the dataset, we are going to download the COCO validation split (because we don't need the full COCO dataset for this),\r\n",
        "then run Deep Dream on it. Finally, there's some further processing we need to do to get it into a format that Pix2Pix will accept for training."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget\r\n",
        "!python make_dataset.py --nimgs 1000 --nvalimgs 250 --destination $topdir/coco-dreamified\r\n",
        "\r\n",
        "# This script will take several hours to complete. If it stops working because AML disconnects,\r\n",
        "# you can call it again, but this time use the following line instead:\r\n",
        "# !python make_dataset.py --nimgs 1000 --nvalimgs 250 --destination $topdir/coco-dreamified --coco $topdir/coco-dreamified --continue"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We have to make sure that image A is the same size as image B\r\n",
        "# I.e., we need to make sure that the dreamified image A is the same size as its corresponding non-dreamified image B.\r\n",
        "!python resize.py $topdir/coco-dreamified"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The Pix2Pix model tutorials all require that you put the dreamified and the raw images into the same image,\r\n",
        "# along the x axis. We'll do that here, using the Pix2Pix repository's script. See it's docstring for license information.\r\n",
        "!python ./combine_A_and_B.py --fold_A $topdir/coco-dreamified/A --fold_B $topdir/coco-dreamified/B --fold_AB $topdir/coco-dreamified-combined --no_multiprocessing"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the results\r\n",
        "import matplotlib.pylab as plt\r\n",
        "%matplotlib inline\r\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 20)\r\n",
        "from PIL import Image\r\n",
        "import os\r\n",
        "import random\r\n",
        "\r\n",
        "# Pick an image at random\r\n",
        "datadpath = os.path.join(topdir, \"coco-dreamified-combined\", \"train\")\r\n",
        "fpaths = [os.path.join(datadpath, fname) for fname in os.listdir(datadpath)]\r\n",
        "random.shuffle(fpaths)\r\n",
        "\r\n",
        "# Display it\r\n",
        "img = Image.open(fpaths[0])\r\n",
        "plt.imshow(img)\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616104705565
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the GAN to Generate Dreamified Images from Real Ones\r\n",
        "\r\n",
        "You should now have a dataset in a folder called \"coco-dreamifed-combined\". Let's make an AML experiment that trains Pix2Pix\r\n",
        "to image translate between the two domains."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the default datasore associated with the current workspace\r\n",
        "from azureml.core import Workspace\r\n",
        "from azureml.core import Dataset\r\n",
        "\r\n",
        "ws = Workspace.from_config()\r\n",
        "datastore = ws.get_default_datastore()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616104777693
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the dataset to the datastore\r\n",
        "import os\r\n",
        "datastore_data_path = \"datasets/coco-dreamified-combined\"\r\n",
        "datastore.upload(src_dir=os.path.join(topdir, \"coco-dreamified-combined\"), target_path=datastore_data_path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616104796679
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the uploaded data as an AML dataset\r\n",
        "dataset = Dataset.File.from_files(path=(datastore, datastore_data_path))\r\n",
        "dataset = dataset.register(workspace=ws, name=\"coco-dreamified-combined\", description=\"COCO after dreamification\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616104804784
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\r\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "ws = Workspace.from_config() \r\n",
        "\r\n",
        "# Choose a name for your compute cluster\r\n",
        "cluster_name = \"gpu1\"\r\n",
        "\r\n",
        "# Verify that the cluster does not exist already\r\n",
        "try:\r\n",
        "    cluster = ComputeTarget(workspace=ws, name=cluster_name)\r\n",
        "    print('Found existing cluster, use it.')\r\n",
        "except ComputeTargetException:\r\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6',  # Make sure to choose something your subscription has access to\r\n",
        "                                                           idle_seconds_before_scaledown=2400,\r\n",
        "                                                           min_nodes=0,\r\n",
        "                                                           max_nodes=1)\r\n",
        "    cluster = ComputeTarget.create(ws, cluster_name, compute_config)\r\n",
        "\r\n",
        "cluster.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616104835532
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is where we are going to put everything\r\n",
        "root_outputs_path = os.path.join(topdir, \"outputs\")\r\n",
        "\r\n",
        "model_path = os.path.join(root_outputs_path, \"model\")\r\n",
        "os.makedirs(model_path, exist_ok=True)\r\n",
        "\r\n",
        "ir_output_path = os.path.join(root_outputs_path, \"intel\")\r\n",
        "os.makedirs(ir_output_path, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616105522353
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the training script from an external repository (under a BSD-like license)\r\n",
        "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git\r\n",
        "%cd pytorch-CycleGAN-and-pix2pix\r\n",
        "!git checkout f13aab8148bd5f15b9eb47b690496df8dadbab0c"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616105544628
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\r\n",
        "from azureml.core import Experiment\r\n",
        "from azureml.core import Environment\r\n",
        "from azureml.core import ScriptRunConfig\r\n",
        "from azureml.core import Dataset\r\n",
        "\r\n",
        "ws = Workspace.from_config()\r\n",
        "dataset = Dataset.get_by_name(workspace=ws, name=\"coco-dreamified-combined\")\r\n",
        "experiment = Experiment(workspace=ws, name=\"dreamification-experiment\")\r\n",
        "\r\n",
        "# We are going to run the train.py script with some args. See the train.py script for license information.\r\n",
        "config = ScriptRunConfig(\r\n",
        "    source_directory=\".\",\r\n",
        "    script=\"train.py\",\r\n",
        "    compute_target=cluster_name,\r\n",
        "    arguments=[\r\n",
        "        \"--dataroot\", dataset.as_named_input(\"input\").as_mount(),\r\n",
        "        \"--name\", \"dreamifier\",\r\n",
        "        \"--n_epochs\", 50,\r\n",
        "        \"--n_epochs_decay\", 25,\r\n",
        "        \"--checkpoints_dir\", \"outputs/checkpoints\",\r\n",
        "        \"--model\", \"pix2pix\",  # Use Pix2Pix instead of CycleGAN\r\n",
        "        \"--direction\", \"AtoB\"  # Train the network to convert raw images to dreamified ones\r\n",
        "    ]\r\n",
        ")\r\n",
        "\r\n",
        "# Set up the training environment.\r\n",
        "env = Environment.from_pip_requirements(name=\"PyTorch-Pix2Pix-Env\", file_path=\"requirements.txt\")\r\n",
        "config.run_config.environment = env\r\n",
        "\r\n",
        "run = experiment.submit(config)\r\n",
        "aml_url = run.get_portal_url()\r\n",
        "print(\"Submitted to compute cluster. Click link below.\")\r\n",
        "print(aml_url)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616172951186
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wait until the experiment run is completed.\r\n",
        "run.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616182592496
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the model from the latest completed run outputs\r\n",
        "import glob\r\n",
        "from azureml.core import Experiment\r\n",
        "\r\n",
        "experiment = Experiment(workspace=ws, name='dreamification-experiment')\r\n",
        "runs = experiment.get_runs()\r\n",
        "\r\n",
        "completed_run = None\r\n",
        "for r in runs:\r\n",
        "    if r.get_status() == 'Completed':\r\n",
        "        completed_run = r\r\n",
        "        break\r\n",
        "\r\n",
        "if completed_run is None:\r\n",
        "    print(\"No completed run available\")\r\n",
        "else:\r\n",
        "    !rm -rf model_path\r\n",
        "    completed_run.download_files(\"outputs\", model_path)\r\n",
        "    print(f'Downloaded model file: {model_path}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616183550608
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to ONNX\r\n",
        "\r\n",
        "In order to convert to ONNX, we'll apply a small patch to one of the files in the repository,\r\n",
        "so that we can get an ONNX model out when we test it. See https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1113"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile \r\n",
        "$topdir/azure-percept-advanced-development/tutorials/pytorch-from-scratch-tutorial/pytorch-CycleGAN-and-pix2pix/0001-Update-base-model.py.patchFrom de494caeb99dcb457c559222277f87b549b90436 Mon Sep 17 00:00:00 2001\r\n",
        "From: Max Strange\r\n",
        "Date: Fri, 19 Mar 2021 14:36:09 -0700\r\n",
        "Subject: [PATCH] Temp Commit\r\n",
        "\r\n",
        "---\r\n",
        " models/base_model.py | 12 ++++++++++++\r\n",
        " 1 file changed, 12 insertions(+)\r\n",
        "\r\n",
        "diff --git a/models/base_model.py b/models/base_model.py\r\n",
        "index 6de961b..304bd3d 100644\r\n",
        "--- a/models/base_model.py\r\n",
        "+++ b/models/base_model.py\r\n",
        "@@ -198,6 +198,18 @@ class BaseModel(ABC):\r\n",
        "                     self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))\r\n",
        "                 net.load_state_dict(state_dict)\r\n",
        " \r\n",
        "+                # Create ONNX model\r\n",
        "+                net.eval()\r\n",
        "+                batch_size = 1\r\n",
        "+                input_shape = (3, 256, 256)  # Default crop size\r\n",
        "+                export_onnx_file = load_filename[:-4]+\".onnx\"\r\n",
        "+                save_path = os.path.join(self.save_dir, export_onnx_file)\r\n",
        "+\r\n",
        "+                dinput = torch.randn(batch_size, *input_shape)\r\n",
        "+                torch.onnx.export(net, dinput, save_path)\r\n",
        "+\r\n",
        "+                print('The ONNX file ' + export_onnx_file + ' is saved at %s' % save_path)\r\n",
        "+\r\n",
        "     def print_networks(self, verbose):\r\n",
        "         \"\"\"Print the total number of parameters in the network and (if verbose) network architecture\r\n",
        " \r\n",
        "-- \r\n",
        "2.30.1\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now apply the patch\r\n",
        "!git apply $topdir/azure-percept-advanced-development/tutorials/pytorch-from-scratch-tutorial/pytorch-CycleGAN-and-pix2pix/0001-Update-base-model.py.patch"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616189969914
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat $topdir/azure-percept-advanced-development/tutorials/pytorch-from-scratch-tutorial/pytorch-CycleGAN-and-pix2pix/models/base_model.py"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test with our Val Split\r\n",
        "\r\n",
        "The generator should be trained. Let's load it in and test it on some example images."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataroot = os.path.join(topdir, \"coco-dreamified-combined\")\r\n",
        "results_dir = os.path.join(topdir, \"outputs\", \"results\")\r\n",
        "\r\n",
        "# Pix2Pix framework wants our val split to be called test split. I've heard it both ways :)\r\n",
        "!mv $dataroot/val $dataroot/test\r\n",
        "!pip install dominate\r\n",
        "!python test.py --dataroot $dataroot --direction AtoB --model pix2pix --name dreamifier --checkpoints_dir $topdir/outputs/model/outputs/checkpoints --results_dir $results_dir --gpu_ids -1"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616190066526
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the results\r\n",
        "import matplotlib.pylab as plt\r\n",
        "%matplotlib inline\r\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 20)\r\n",
        "from PIL import Image\r\n",
        "import os\r\n",
        "import random\r\n",
        "\r\n",
        "# Pick an image at random\r\n",
        "images_dir = os.path.join(results_dir, \"dreamifier\", \"test_latest\", \"images\")\r\n",
        "fpaths = [os.path.join(images_dir, fname) for fname in os.listdir(images_dir) if fname.endswith(\"_fake_B.png\")]\r\n",
        "random.shuffle(fpaths)\r\n",
        "\r\n",
        "# Display a few\r\n",
        "nimgs = 5\r\n",
        "for i in range(0, nimgs * 2, 2):\r\n",
        "    # Input image on the left\r\n",
        "    imgname = os.path.basename(fpaths[i])\r\n",
        "    rawfpath = os.path.join(images_dir, imgname.replace(\"_fake_B.png\", \"_real_A.png\"))\r\n",
        "    raw = Image.open(rawfpath)\r\n",
        "    plt.subplot(nimgs, 2, i + 1)\r\n",
        "    plt.imshow(raw)\r\n",
        "\r\n",
        "    # Dreamified image on the right\r\n",
        "    img = Image.open(fpaths[i])\r\n",
        "    plt.subplot(nimgs, 2, i + 2)\r\n",
        "    plt.imshow(img)\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616190701835
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check that the ONNX Model Works Too"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install onnxruntime\r\n",
        "\r\n",
        "import os\r\n",
        "import onnx\r\n",
        "onnx_model_path = os.path.join(topdir, \"outputs\", \"model\", \"outputs\", \"checkpoints\", \"dreamifier\", \"latest_net_G.onnx\")\r\n",
        "onnx_model = onnx.load(onnx_model_path)\r\n",
        "onnx.checker.check_model(onnx_model)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616191364430
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate with the ONNX model now\r\n",
        "import matplotlib.pylab as plt\r\n",
        "%matplotlib inline\r\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 20)\r\n",
        "from PIL import Image\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import onnxruntime\r\n",
        "import torch\r\n",
        "\r\n",
        "# Make predictions with our ONNX model\r\n",
        "def to_numpy(tensor):\r\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\r\n",
        "ort_session = onnxruntime.InferenceSession(onnx_model_path)\r\n",
        "\r\n",
        "nimgs = 5\r\n",
        "for i in range(0, nimgs * 2, 2):\r\n",
        "    # Input image on the left\r\n",
        "    imgname = os.path.basename(fpaths[i])\r\n",
        "    rawfpath = os.path.join(images_dir, imgname.replace(\"_fake_B.png\", \"_real_A.png\"))\r\n",
        "    raw = Image.open(rawfpath)\r\n",
        "    plt.subplot(nimgs, 2, i + 1)\r\n",
        "    plt.imshow(raw)\r\n",
        "\r\n",
        "    # Run the raw image through the model and display on the right\r\n",
        "    # We need to do some pre and post processing to make the network happy\r\n",
        "    # First, convert the raw image from Numpy to Torch, BCWH format, and then convert to float.\r\n",
        "    raw_as_tensor = torch.from_numpy(np.asarray(raw)).permute(2, 0, 1).unsqueeze(0).type(torch.FloatTensor)\r\n",
        "    # Now go from [0, 255] -> [-1.0, 1.0]\r\n",
        "    raw_as_tensor = (raw_as_tensor - 127.5) / (127.5)\r\n",
        "    # Create an ONNX RT session and feed it the tensor image\r\n",
        "    ort_output = ort_session.run(None, {ort_session.get_inputs()[0].name: to_numpy(raw_as_tensor)})\r\n",
        "    # Remove the batch dimension, permute it back to WHC format, and then convert to int, range [0, 255]\r\n",
        "    # The output of this model is [-1.0, 1.0]\r\n",
        "    img = ort_output[0].squeeze(0)\r\n",
        "    img = torch.from_numpy(img).permute(1, 2, 0).numpy()\r\n",
        "    img = np.round((img + 1) * 255 / 2)\r\n",
        "    print(\"Max:\", np.max(img), \"Min:\", np.min(img))\r\n",
        "    img = img.astype(np.uint8)\r\n",
        "    plt.subplot(nimgs, 2, i + 2)\r\n",
        "    plt.imshow(img)\r\n",
        "\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616196746046
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # We need this for converting to OpenVINO\r\n",
        "    input_name = ort_session.get_inputs()[0].name\r\n",
        "    output_name = ort_session.get_outputs()[0].name"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616198160674
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to OpenVINO"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First convert from ONNX to IR\r\n",
        "!rm -rf $topdir/openvino/mnt\r\n",
        "!mkdir -p $topdir/openvino/mnt\r\n",
        "!cp $onnx_model_path $topdir/openvino/mnt/model.onnx\r\n",
        "!docker run --rm -v $topdir/openvino/mnt:/mnt -w /mnt openvino/ubuntu18_dev:2021.1 \\\r\n",
        "    python3 \"/opt/intel/openvino_2021/deployment_tools/model_optimizer/mo.py\" \\\r\n",
        "    --input_model \"./model.onnx\" -o \"./\" --input $input_name --output $output_name --scale 127.5 --mean_values \"(127.5, 127.5, 127.5)\"\r\n",
        "!cp $topdir/openvino/mnt/model.bin $ir_output_path\r\n",
        "!cp $topdir/openvino/mnt/model.xml $ir_output_path"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616198345228
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls $ir_output_path"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}